What learning algorithms can predict that our physics theories might not

A [hot topic] in physics right now is applying machine learning algorithms and techniques from information theory to better understand physics. [http://fqxi.org/community/essay/winners/2013.1] There are even hopes that these techniques might develop into a so-called "theory of everything" that unifies and explains all aspects of the universe. But for all the attention this subject has gotten, I'm a bit disappointed that I've never seen an introduction that explains

1. where *exactly* might you be able to use machine learning algorithms where you can't just use [existing] [physics] theories instead, and
2. a hands on guide to applying learning algorithms in these situations, for people who don't know information theory jargon

So I'm going to write one!

People often explain information physics or digital physics by saying it is something about the entropy of black holes [http://www.pbs.org/wgbh/nova/blogs/physics/2014/04/is-information-fundamental/], or worse, the idea that we might be living in a computer simulation [http://www.wired.com/2002/12/holytech/]. I think this does little to tell people what information physics is actually about, so I'm not going to do that. Instead, I think a better approach is to start with the following idea:

> *If* it is possible to put ourselves inside computer simulations, it *might* be possible to put ourselves in a situation where we cannot use existing physics theories to predict what we'll perceive happening to us, but where we can use machine learning algorithms to do so.

But before we get into it, let me be very clear about something. Currently, information physics is just a *hypothesis* about how the universe works. Unlike more established theories such as the [standard model] and [general relativity], we do not know if this is *actually representative* of how the universe works, or even if a new physics theory is needed to explain the situation hinted at above. (Note the emphasis on "if" and "might.") Answering those questions would require serious experimental evidence that is way beyond our capabilities today. But given how quickly our technology is improving, I wouldn't be completely shocked if it is possible in my lifetime.

So first question: how might we be able to put ourselves in a computer simulation?

Putting ourselves in a computer simulation

If you haven't been living under a rock, you've probably noticed that computers and software have gotten *way* better over the last several decades, and this trend shows no signs of stopping. So people have tried to predict how long it would take until computers become as smart as humans, making a ton of dubious assumptions in the process, but according to [one study], most so-called "experts" who've done that ended up with a prediction between 2020 and 2060.

[http://library.fora.tv/2012/10/14/Stuart_Armstrong_How_Were_Predicting_AI at 10:15]
[there is a long tail but majority within 2060, see http://hplusmagazine.com/2010/02/05/how-long-till-human-level-ai/]

If they're right, we might see super-smart computers in my lifetime! But even if they're wrong, it's fun to speculate about what really powerful computers would be capable of anyway.

Virtual reality simulations are already pretty good these days. If computers in 2060 might be smarter than humans, then virtual reality simulations in 2060 might be so good that to a simulated person living in one, it would look pretty convincingly like a real universe.

Instead of speculating about it, could we see for ourselves if that is the case? Perhaps it could be done like this. People have used brain-to-brain interfaces to think to each other over the internet, and the technology to do things like this is improving quickly. [https://www.washington.edu/news/2015/09/23/uw-team-links-two-human-brains-for-question-and-answer-experiment/] It wouldn't be too much of a stretch to imagine that several decades from now, these brain-to-brain connections might have so much bandwidth that people hooked up this way would feel like a single person. After all, multicellular organisms evolved from single-celled organisms, and you can think of ourselves as a bunch of single-celled organisms hooked up in such a way that they're dependent on each other. Despite that, we each still feel like a single person. So how would it feel if we hooked up two people's brains together in year 2060?

[I'm him!]

Better yet, what if we hooked up your brain to a simulated brain in a virtual reality world? Would you feel like you are also the person in the simulation?

[I'm him!]

We won't know until we actually try it. If the answer is no then the rest of this blog post is completely irrelevant, but if the answer is yes then that would be really cool!

So let's say you do that and you're now a single person that lives in both the real world and a virtual reality world. Then the real world part of you gets into a tragic accident and your brain is blasted into a million pieces. How sad.

[blowing up real world brain]

But are you really dead? Because, you know, people have done surgeries where they removed half of someone's brain and they still lived on OK. [https://en.wikipedia.org/wiki/Hemispherectomy] So if half of your brain is in the real world and half of your brain is in a computer simulation and you lose the part of your brain in the real world, shouldn't you still live on in the simulation?

If you try it out and the answer is no, then the rest of this blog post is completely irrelevant. But if the answer is yes, then it means that it's possible to put ourselves completely inside a computer simulation! Isn't that like the coolest thing ever? (If you like this sort of thing, here's a Wait But Why post describing [more situations like these](https://waitbutwhy.com/2014/12/what-makes-you-you.html).)

So what does this have to do with information physics? Oh right:

> *If it is possible to put ourselves inside computer simulations*, it might be possible to put ourselves in a situation where we cannot use existing physics theories to predict what we'll perceive happening to us, but where we can use machine learning algorithms to do so.

So now I've explained the first part. But even if you can live completely inside a simulation (and this is already a huge if), the computers running the simulation must still follow the laws of physics. So if you know how the simulation is programmed to behave, you don't need machine learning algorithms to predict what will happen to you. And if you could somehow know [general relativity], the [standard model], and the state of every particle in the universe at the time your real world brain is blasted into a million pieces, then even if someone later decides to change the computer code of the simulation, then in theory you could predict that too. So what do you need learning algorithms for?

What our physics theories can't predict

One really cool property of computer simulations is that you can make exact copies of them on different computers. If the simulation uses what's called a "deterministic algorithm," then it means that all copies of the simulation will output the same thing.

Now let's suppose that after disconnecting your real world brain, the simulation you're living in becomes deterministic, and that 30 seconds from now, it's programmed to display a message to you saying "You're still alive!" But those of us controlling the simulation are evil pranksters, and we decide to play a trick on you. Once the simulation becomes deterministic, we copy it onto a second computer, and right before it displays the "You're still alive!" message, we blast the original computer into a million pieces. Now here's the question: do you get to see the message?

[drawing]

Well, it depends on how we define "you," doesn't it? If we define "you" based on which physical body you're in, or which physical computer you're in, then you "die" in the original computer and don't get to see the message. (Actually, by that definition, one can argue that you die much earlier, when your physical brain is blasted into a million pieces.) But if we instead define "you" in a different way where it doesn't matter which physical body or computer you're in, then maybe you do get to see the message.

But here's the thing. The way science works is that you're supposed to do experiments to figure out what's right and what's wrong. And what we just described here is a way for you to *experimentally determine* which of these definitions of "you" is wrong! How? By using the procedure above to attempt to put yourself in a computer simulation, then having some friends do that trick so that only a copy of the simulated "you" gets to see the "You're still alive!" message. If you die before you see the message, then the definitions of "you" predicting that you see the message are wrong. If you *do* get to see the message, then you know for a fact that the definitions of "you" predicting that you do *not* see the message are most definitely wrong! (One weird thing about this experiment is that people watching the computers from outside don't get to find out which definitions of "you" are wrong, but they can still reproduce the outcome for themselves by putting themselves inside a simulation.)

I'm going to use the terminology of the [Wait But Why post] I linked to earlier, and I'm going to call the hypothesis that you *do not* see the message the "brain theory." And I'm going to call the hypothesis that you *do* see the message the "data theory."

Let's say the data theory is right that if you do the experiment, you do in fact see the message. Well, that means we get to do an even crazier experiment! Suppose we do the same experiment as above, except for the following:

1. The evil pranksters copy the simulation onto 2 other computers, instead of just 1.
2. The simulation is deterministic, except for one thing: if someone presses a key on the keyboard, you get to see what they pressed from inside the simulation.
3. After the first computer is destroyed and the "You're still alive!" message appears, the evil pranksters press the `0` key on the second computer, and simultaneously press the `1` key on the third computer.

Do you see a `0` or a `1`?

[drawing]

Easy, right? Isn't there just a 50% chance that you see `0` and a 50% chance that you see `1`? Not so fast. Just because there's 2 possibilities doesn't necessarily mean that each one has a 50% chance of occurring. For example, if you consider the possibilities of whether or not you'll be struck by lightning within the next 10 seconds, it is far more likely that you will not be struck by lightning.

Well, then, how about we do this the proper way, and use the fundamental laws of physics to derive what's going to happen? Well, the most fundamental physics theories we know of today are the [standard model of particle physics] and [general relativity]. And if you do the painstaking calculations, they'll tell you that the outcome of this experiment looks something like this:

[dead person]
[dead person in computer]
[person that sees 0]
[person that sees 1]

Here's the thing. While these theories can give you a probability distribution over what every atom in those computers is going to do, neither of them has a concept of who "you" are. The closest thing we have to that is the idea from quantum mechanics that you can't tell where a particle is until you measure it, but ironically, defining what exactly is a "measurement" is considered a [great unsolved problem in physics](https://en.wikipedia.org/wiki/Measurement_problem).

Of course, if the brain theory is right and you don't see the "You're still alive!" message in the first place, then none of this matters. But if the data theory is right and you *do* see the "You're still alive!" message, then that would mean *it's possible to do an experiment in which the most fundamental physics theories we know of today cannot predict the probability of you seeing a `0` or a `1`*. In that case, we'd need a new physics theory to fill the gap.

Approaching the problem

By the time we have the technology to perform these experiments, scientists may or may not have a pretty good guess of whether the brain theory or data theory is right, and (if the data theory is right) how to predict the probability of seeing a red or green flash [change to `0` or `1`] in the situation above. But they wouldn't be able to tell us if their guesses are correct because even if they've done the experiment before, they would've seen both the red flash in the second computer *and* the green flash in the third computer! Since the first time you'll be able to compare their predictions to observations will be when you're in the simulation yourself, you should be very open to the possibility that their guesses will be wrong. So how could you check whether they are wrong?

Seeing a single flash would not really be enough to check their predictions, so let's modify the setup. Similar to before, we put you in a computer simulation, pause the simulation, copy it onto a second computer, resume it on both computers, then flash red in the first computer and green in the second computer. (Previously I described a variation that used 3 computers and destroyed the original computer, but I don't think that's necessary here.) But this time we don't stop there. Next, we do the same thing to both computers. In other words, if we flashed red in computer A and green in computer B, we copy computer A's state to a new computer C, copy computer B's state into a new computer D, flash red in computers A and B, and flash green in computers C and D. Then we keep repeating this process of copying the simulation states into new computers, flashing red in the originals and flashing green in the copies, until we run out of computers or funding or whatever.

[picture]

If the scientists tell you a way to predict the probabilities of seeing a red flash or green flash, you can use that to make predictions and check them as you see each flash. If the sequence of flashes you see is consistent with your predictions, then great! But if not (which is likely considering that scientists that didn't put themselves in simulations could not have checked the predictions themselves), then if you believe in the scientific method, you should come up with a new way to predict the color of each flash, and if your new predictions are wrong, you should keep refining your theory until you have one that seems to work.

But wait a moment! If the scientists' original way of predicting flashes is so likely to be wrong, then why even learn it in the first place? It almost seems like the most useful thing to teach you in this situation would not people's initial guesses of how you can predict whether you'll see a red or green flash, but rather how you can quickly learn from the results once you start measuring them.

The most badass learning algorithm ever invented

Machine learning is a really big field, and there are way too many learning algorithms to cover them all here. Most of these algorithms exist either to address practical, real-life problems such as performance or perceived theoretical problems such as runtime complexity. But if you are a godlike being who is not concerned with either of these worldly problems (and let's face it, we all wish we were), there is really only one learning algorithm you need to know, and it is called Solomonoff induction. Mathematicians have more or less proven that it is the best general purpose learning algorithm theoretically possible that we can be approximately compute using known physics, and I think it is the most badass learning algorithm ever invented.

Here's the problem that Solomonoff induction is designed to solve: suppose there is some unknown process generating a sequence of 1's and 0's (also known as a bitstring) and you want to guess the next bits in the sequence. The 1's and 0's can represent anything you want, such as whether each flash is red or green, or perhaps English text (by splitting up the bitstring into blocks that each correspond to a letter of the alphabet).

So knowing how the bitstring begins, how can you try to guess the unknown bits that follow? Intuitively you'd think that they probably continue the same pattern as the previous bits, rather than being something "random." But how can you tell how "random" a bitstring is? A subjective but really effective way of doing that is by figuring out what are the lengths of the shortest computer programs that output that bitstring. So let's use this idea to assign a probability to every bitstring.

First we need a way to randomly generate computer programs that produces shorter programs more often than longer programs. Easy! Take 2 fair coins and flip them. If the first coin is heads then write a 0; otherwise write a 1. If the second coin is heads then flip both coins again; otherwise stop. The resulting bitstring is our *computer program*. (Remember that text such as computer code is really just a bitstring.) [allow 0 length programs like in busy beaver problem]

Now we need this computer program to output a bitstring. Simple! Save the computer program as a binary file and run it using a deterministic interpreter for your favorite programming language, perhaps Python or JavaScript. (It doesn't matter which language, as long as it's Turing complete and has something like print or console.log that prints to the console. Also you should let the program use unlimited time and memory.) Of course, almost all randomly generated computer programs will have a syntax error, but that's OK because there is still a chance that the program will run and print something. As for the text, I mean bitstring, that the program prints to console, how about we call it the *console output*.

Finally we can assign a probability to every bitstring. If you generate and run random computer programs, over and over again for the rest of your life and a long time beyond, we can define the *algorithmic probability* of a bitstring to be the number of *console outputs* so far (counting duplicates) that *begin* with the bitstring in question, divided by *total* number of console outputs so far (both of which approach infinity). Since a sequence of colors can be represented as a bitstring, you can use this formula to calculate the probability that you will see any sequence of red and green flashes when you put yourself in a 2060-era virtual reality simulation!

[are there any issues caused by finite console outputs?]

Now that's cool, but what we really want to do is predict which colored flash you'll see *next*. To do that, first write down a bitstring corresponding to the sequence of flashes you've seen so far (you can use 0 for red and 1 for green), followed by an extra 0 at the end. I'll call that bitstring R. Also write down a bitstring corresponding to the flashes you've seen so far, followed by an extra 1 at the end, which I'll call bitstring G. Then according to *Solomonoff induction*, the probability of seeing a red flash next is just the algorithmic probability of bitstring R, divided by the sum of the algorithmic probability of bitstring R and the algorithmic probability of bitstring G. The probability of seeing a green flash next is just one minus the probability of seeing a red flash next.

Phew, that was a mouthful! If you like, here's a flowchart:

the
extremely patient [1] person's
guide to
THE MOST BADASS LEARNING ALGORITHM
EVER INVENTED

  Start
    |
    v
*Flip* 2 coins <---  <-------------------------
    |              |                           |
    v              |                           |
If first coin is:  |                           |
[heads] -> write 0 |                           |
[tails] -> write 1 |                           |
    |           ---                            |
    |    Second coin is [heads]                |
    | Second coin is [tails]                   |
    v                                          |
Save bits as a *binary file*                   |
100101 -> random_program.bin                   |
    |                                          |
    v                                          |
*Execute* it as a computer program [2]         |
python random_program.bin > console_output.bin |
    |                                          |
    v                                          |
When it starts executing,                      |
get a new sheet of paper and...                |
    |                        ------------------
    |                   Do until the end of time
    | When time ends
    v
Given a bitstring *B*:
                                 # of console outputs (counting duplicates) that begin with *B*
algorithmic probability of *B* = --------------------------------------------------------------
                                 total # of console outputs (counting duplicates)
    |
    v
Given a bitstring *C*:
                                  algorithmic probability of *C* followed by 0
probability that next bit is 0 = -----------------------------------------------
                                 (algorithmic probability of *C* followed by 0 +
                                  algorithmic probability of *C* followed by 1)

probability that next bit is 1 = 1 - probability that next bit is 0

[1] "Extremely patient" is like the biggest understatement ever.
[2] Use the same deterministic Turing complete interpreter for every program, and let it use unlimited time and memory. In this example, I assume Python is running in a completely deterministic virtual machine.

Isn't that completely ridiculous? But that, folks, has been mathematically proven to be the best general purpose learning algorithm theoretically possible, assuming you have unlimited computing power and patience.

If you want to learn more about Solomonoff induction and why it is so badass, see here, here, and here.
[http://lesswrong.com/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/]
[http://scholarpedia.org/article/Algorithmic_probability]
[http://twistedoakstudios.com/blog/Post5623_solomonoffs-mad-scientist]

Can we do better?

Even though the procedure I just described uses the most badass learning algorithm ever invented, it still has an important limitation. If you follow it precisely, you will only make predictions using knowledge of past flashes, ignoring knowledge of other stuff. Suppose that for some really weird reason, whenever you wave your arms and shout "I will see a red flash," it increases your chances of seeing a red flash next. (You never know when the flash colors are chosen by an unknown process.) That would be a good thing to know, but the procedure above would not learn to take that into account. Could we modify it to take into account information like that?

Yes we can! Instead of writing down a bitstring corresponding to the sequence of flashes so far and using Solomonoff induction to predict just the color of the next flash, you can record a video of everything you see and hear (and smell and touch and taste) and use Solomonoff induction to predict how the video will continue, including colors of future flashes. This is possible because videos are ultimately stored on your computer as 1's and 0's. In fact, there's an added bonus of doing it this way (if you can ignore the even worse performance issues). When the bitstring only contains the sequence of flashes, Solomonoff induction would not predict other things you will perceive in the simulation and you would need a separate way of doing that, such as knowing how the simulation is programmed. But if you use Solomonoff induction to predict *everything* you will sense in the future, you would no longer need to rely on someone telling you how the simulation behaves before you put yourself in the simulation.

Heck, in theory you could do that to predict what will happen to you right now in the real world, even if you do not put yourself in a simulation. Which raises the question of whether learning algorithms should be considered physics theories in their own right. (More on that in future blog posts, if I get the time to write them.)

What's the point?

I've wanted to write this post since mid-2014 (it's now January 2016), but with everything else going on it took me longer than expected to learn how to approach the questions in this post, then to figure out how to explain this stuff to friends. So why does this impractical-sounding stuff about living in a 2060-era computer simulation matter?

[email, CC0]
